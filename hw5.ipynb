{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Section\n",
    "In this homework, we will cover:\n",
    "\n",
    "1. Some basic statistics for relationships describing pairs of random variables\n",
    "1. Linear regression\n",
    "1. Assessing the quality of model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import modules needed for homework\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression as lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief overview of useful statistics\n",
    "\n",
    "Given $n$, paired, samples $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$, of the random variables $X,Y$, we may want to deduce from this data whether $X$ and $Y$ seem to be related in some way.  Thankfully, there are certain statistics that can be generated from this data to help answer this question.  \n",
    "\n",
    "Sample **mean**: \n",
    "\n",
    "$$\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{N}$$\n",
    "\n",
    "Tells us the expected value of a variable\n",
    "\n",
    "Sample **variance**: \n",
    "\n",
    "$$var(X) = \\frac{\\sum_{i=1}^n (X_i-\\bar{X})^2}{N}$$\n",
    "\n",
    "Tells us the average squared deviation about the variable's mean (spread of variable)\n",
    "\n",
    "Sample **covariance**: \n",
    "\n",
    "$$cov(X, Y) = \\frac{\\sum_i (X_i-\\bar{X}) (Y_i - \\bar{Y})}{N}$$\n",
    "\n",
    "Tells us how the extent to which two variables are related, or the extent to which two variables influence one another.  Large, positive covariance means that if one variable\n",
    "gets larger or smaller, so does the other.  Large, negative covariance means that one variable being larger or smaller implies the other is smaller or larger.  If random variables are independent, their covariance is zero, but the reverse is not always true.\n",
    "\n",
    "**Note**:  The covariance of a variable with itself is defined as it's variance, however, due to the precise way these statistics are computed in numpy, these quantities will not be exactly equal.\n",
    "\n",
    "Sample **correlation**: \n",
    "\n",
    "$$corr(X, Y) = \\frac{cov(X,Y)}{\\sqrt{var(X)}\\sqrt{var(Y)}}$$\n",
    "\n",
    "Though covariance is useful for determining whether or not variables are related, it's hard to interpret, because it lacks a sense of scale.  Correlation is a normalized version of covariance, which is constrained to lie between -1 and 1.\n",
    "\n",
    "**Note**:  When computing variance and covariance, we will sometimes normalize by N-1 instead of N.  Which one is \"correct\" typically depends on whether the sample mean $\\bar{X}$ is known, or if it is also being estimated.  You are free to use whichever you prefer.  Note numpy uses N by default, while pandas uses N-1.  This is the reason behind the previous note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of X:  0.0034208641945705437\n",
      "variance of X:  0.9825914135417461\n",
      "cov matrix of X,Y: \n",
      " [[ 0.98268968 -0.0016514 ]\n",
      " [-0.0016514   0.99143065]]\n",
      "covariance of X,Y:  -0.0016513960105007384\n",
      "correlation matrix of X,Y: \n",
      " [[ 1.         -0.00167306]\n",
      " [-0.00167306  1.        ]]\n",
      "correlation between X,Y:  -0.0016730613341107998\n"
     ]
    }
   ],
   "source": [
    "##Computing statistics in numpy\n",
    "\n",
    "##Generate some data\n",
    "##np.random.normal(mu,sigma,n) generates n samples of a normally distributed random variable\n",
    "##with mean mu and standard deviation sigma\n",
    "\n",
    "##two seperate arrays of data\n",
    "X = np.random.normal(0,1,10000)\n",
    "Y = np.random.normal(0,1,10000)\n",
    "\n",
    "##mean and var of 1d array\n",
    "print('mean of X: ', np.mean(X))\n",
    "print('variance of X: ', np.var(X))\n",
    "\n",
    "##cov of 2 1d arrays\n",
    "##this actually returns a matrix, whose i-jth entry is cov(variable i,variable j)\n",
    "print('cov matrix of X,Y: \\n', np.cov(X,Y))\n",
    "print('covariance of X,Y: ', np.cov(X,Y)[0,1])\n",
    "\n",
    "##correlation between 2 1d arrays\n",
    "##note that again, this returns a matrix, whose i-jth entry is corr(var i, var j)\n",
    "print('correlation matrix of X,Y: \\n',np.corrcoef(X,Y))\n",
    "print('correlation between X,Y: ',np.corrcoef(X,Y)[0,1])\n",
    "##note the correlation of a variable with itself is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $cov(X,X)$ does not equal $var(X)$, due to division by $N-1$ in the former, and $N$ in the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.65847193 -0.54979746]\n",
      " [ 1.56511553  1.0127544 ]\n",
      " [-0.85496099 -0.18242849]\n",
      " [-0.6526211   0.8003411 ]\n",
      " [-0.35208134 -0.76577456]]\n"
     ]
    }
   ],
   "source": [
    "##Lets do another example, but with each variable stored as a column in a matrix\n",
    "##Call X the first colum, Y the second\n",
    "Z = np.random.normal(0,1,[10000,2]) \n",
    "print(Z[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var of X:  0.9661850251899406\n",
      "var of Y:  0.9920240751781412\n",
      "var of X and var of Y:  [0.96618503 0.99202408]\n",
      "cov matrix of X,Y: \n",
      " [[0.96628165 0.00247759]\n",
      " [0.00247759 0.99212329]]\n",
      "correlation matrix of X,Y: \n",
      " [[1.         0.00253044]\n",
      " [0.00253044 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "##get variances, correlations, covariances of columns of Z\n",
    "print(\"var of X: \", np.var(Z[:,0]))\n",
    "print('var of Y: ', np.var(Z[:,1]))\n",
    "print('var of X and var of Y: ', np.var(Z,axis=0))\n",
    "\n",
    "##numpy cov function has an annoying construction\n",
    "##it assumes variables correspond to rows, so we need to flip, or transpose, the matrix\n",
    "##transpose of a matrix is a new matrix with Z[i,j].T = Z[j,i]\n",
    "\n",
    "\n",
    "print('cov matrix of X,Y: \\n', np.cov(Z.T)) \n",
    "\n",
    "##correlation matrix (need to flip again)\n",
    "print('correlation matrix of X,Y: \\n', np.corrcoef(Z.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "The goal of most scientific pursuits is the creation of a model which can adequately describe the relationship between one or more dependent (outcome) variables and one or more independent variables (features, or inputs).  While these models are often used to generate predictions of outcomes, given inputs, they can also be used to better understand how quantities are related, i.e., to infer what input variables meaningfully impact a given outcome.\n",
    "\n",
    "\n",
    "For now, we will only consider a simple case in which we want to predict one output variable $Y$ from a single input variable $X$.  We start by assuming that there is some relationship between these two variables:\n",
    "\n",
    "$$Y = f(X)$$\n",
    "\n",
    "and aim to construct some model for, or estimate of, $f(X)$, $\\hat{f}(X)$.  \n",
    "\n",
    "\n",
    "Linear regression starts by assuming what is basically the simplest possible model for $f(X)$, i.e., a linear model:\n",
    "\n",
    "$$Y = \\hat{f}(X) = aX + b$$.\n",
    "\n",
    "Naturally, such a simple model is unlikely to perfectly describe the relationship between $\\textit{any}$ two variables, moreover, it is almost certainly the case that there are other variables that influence $Y$ which are not included in our model.  To account for these shortcomings, linear regression models also include a random component, which is intended to represent the influence of other, external, variables on our outcome, or to account for uncertainty in the specification of our linear model. The full linear regression model is then:\n",
    "\n",
    "$$Y = \\hat{f}(X) + \\epsilon = aX + b + \\epsilon ,$$\n",
    "\n",
    "where $\\epsilon$ is a random variable, commonly referred to as a noise or error term.  While other choices are possible, for now we assume that $\\epsilon$ is a normal (Gaussian) random variable, with mean $0$ and variance $\\sigma^2$, $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n",
    "\n",
    "A crucial assumption of linear regression is that our noise terms are independent, and identically distributed, across distinct observations of $X$ and $Y$.  More precisely, given $n$ observations of $Y$, $y_1,y_2,...,y_n$, along with $n$ paired inputs $x_1,x_2,...,x_n$, and our model:\n",
    "\n",
    "$$y_i = ax_i + b + \\epsilon_i,$$\n",
    "\n",
    "we assume $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ for all $i$, where $\\mathcal{N}(0,\\sigma^2)$ is shorthand for a normal random variable, with mean 0 and variance $\\sigma^2$.\n",
    "\n",
    "Given data, one \"fits\" a linear regression model by finding values for the slope $a$ and intercept $b$ that best explains the observed data.  For now, you don't need to understand the mechanics of this estimation procedure, however, if you're interested, you can read more at https://en.wikipedia.org/wiki/Ordinary_least_squares . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated slope:  [1.98698074]\n",
      "Estimated intercept:  0.986671442938197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.00071967e+00, -1.72497517e+00,  2.69394160e+00,  2.28070984e+00,\n",
       "        3.42271030e+00,  8.19585962e-01, -4.67851270e+00,  6.23846007e-01,\n",
       "        1.09377506e+00, -9.32443226e-01, -9.96752144e-01,  4.58174485e+00,\n",
       "       -1.43195817e+00,  3.45206226e+00,  2.28031945e+00,  7.38786040e-01,\n",
       "        1.62410442e+00,  3.23401901e+00, -3.82426399e-01, -3.54254168e+00,\n",
       "       -1.47827722e+00, -1.05401044e+00, -5.93506860e-01,  2.60673167e+00,\n",
       "        3.72994905e+00,  2.47852748e+00, -3.72380552e+00, -3.61569779e+00,\n",
       "       -1.56641058e+00,  9.44354634e-01, -1.96659531e+00,  4.76548408e-01,\n",
       "        1.56746778e+00,  1.91012165e+00,  2.89765451e+00,  1.52672728e+00,\n",
       "       -5.22639159e-01,  1.26228142e+00, -3.86738158e-01,  4.05568066e+00,\n",
       "        2.22297981e+00,  2.00806212e+00,  4.41567763e+00, -1.81014957e+00,\n",
       "        8.61925970e-01,  6.24989195e+00,  5.03753912e-01,  4.86733092e-01,\n",
       "        2.52534742e+00,  2.44605880e+00,  3.22429347e+00, -1.39931870e+00,\n",
       "       -1.16685973e+00, -1.87218162e+00,  2.70399327e+00,  2.26901132e+00,\n",
       "        1.38092001e+00, -1.53543743e-01,  3.44742420e+00,  8.32205992e-01,\n",
       "        3.11876775e+00,  6.68132120e-01,  2.72561022e+00, -1.20243442e+00,\n",
       "        1.52110374e+00,  1.01353715e+00, -2.94615349e+00, -6.85426669e-01,\n",
       "        9.19713706e-01,  2.51094940e+00,  1.32553017e+00, -7.25189563e-01,\n",
       "       -1.31507442e-01,  2.14256470e-01,  2.30749703e+00, -8.63937991e-01,\n",
       "       -1.49013939e+00,  3.17584400e+00,  3.48761055e+00,  8.17221348e-01,\n",
       "        2.25456173e+00, -4.27512519e-01,  4.32169749e-02, -2.04592456e+00,\n",
       "        4.33497110e+00,  4.57586310e+00, -9.57696119e-01,  2.15924093e-01,\n",
       "        2.58079666e+00, -1.61306972e+00,  3.07047915e-03, -1.65427383e-01,\n",
       "        2.22757438e+00, -1.28747081e+00,  2.39446701e+00,  1.55930660e+00,\n",
       "        1.46864075e+00, -1.52619771e+00,  2.40188362e+00,  2.62796267e+00])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Fitting a linear regression model in sklearn is as easy as typing .fit\n",
    "\n",
    "##make data\n",
    "X = np.random.normal(0,1,100) \n",
    "y = 2*X + 1 + np.random.normal(0,.1,100) ##add some noise so its not too easy\n",
    "\n",
    "##create a linear regression object\n",
    "my_linear_model = lr() \n",
    "\n",
    "##fit it to data (find a and b)\n",
    "##note, sklearn requires the input X to be n by 1, rather than an array of length n\n",
    "##numpy arrays can be dynamically reshaped using the .reshape function\n",
    "##-1 stands for \"as long as you can make it\"\n",
    "##so if you have n elements in an array, -1,1 will make it n by 1.\n",
    "my_linear_model.fit(X.reshape(-1,1),y) \n",
    " \n",
    "##what are model estimates for a and b?\n",
    "print('Estimated slope: ', my_linear_model.coef_)\n",
    "print('Estimated intercept: ',my_linear_model.intercept_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, linear regression was a good model because the relationship between $X$\n",
    "and $Y$ was linear.  The estimated parameters are near their true values.\n",
    "\n",
    "To assess how well a linear regression fits data without prior knowledge of how $X$ and $Y$ are related, we typically train, or fit, the model, using a subset of our data, then see how accurate its predictions are on a holdout set of testing data.  For now, we'll just see how\n",
    "accurately it can predict the data on which it was fit.\n",
    "\n",
    "To do this, we will generate model predictions.  Given a test point $x^*$, predictions under our model will be given: \n",
    "\n",
    "$$y^* = \\hat{a}x^* + \\hat{b},$$\n",
    "\n",
    "where $\\hat{a},\\hat{b}$ are our esimated values of $a$ and $b$ generated during our call to .fit().  These predictions can be generated by calling the .predict method of sklearn `LinearRegression` objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.62702727 -0.495831   -1.12073485  3.17099545 -0.8126051  -0.65910152\n",
      " -0.81610224  0.78848837 -0.34726483  1.87834353  1.06166277  1.11221548\n",
      "  2.54040135  2.01357396 -0.56494191  2.93457033  3.11844485  1.58673592\n",
      " -0.33992335  1.26814655  0.95527122 -1.22430472  0.257062   -0.9482677\n",
      "  2.50538821 -0.68533645  0.1245509  -0.31899427  0.66571543  1.91768122\n",
      "  3.42142847  1.16729859  1.0028017  -0.10391111  0.63157634  2.46647464\n",
      "  0.36047464 -0.00654492 -0.69998642  0.0808147   2.72437486  2.33137105\n",
      "  5.0643319  -0.7152455   2.86892965  5.06007077  4.10512244  0.55129999\n",
      " -0.57528597  0.11046676  3.27449012  1.19278405  2.86958719  0.34158776\n",
      "  1.58562861  1.38973903  0.54662168  2.94220558 -1.87358781  0.81101856\n",
      "  3.88105027 -0.51044914 -0.29478367 -0.49387119  0.40226012  1.42754825\n",
      " -0.40095773 -3.20133012 -0.29080235  0.68975596  0.61711122  0.04862763\n",
      " -1.06624403  1.45700596  0.18743792  2.16806276  0.72510701 -0.40547598\n",
      "  6.11370496  1.95672186  2.13177967 -0.20560382 -1.45023827 -1.12680134\n",
      "  3.2987194  -0.78404809 -0.56925043 -1.29398489  2.50588895  0.44762228\n",
      "  2.07239895  0.18780849  2.70674848 -1.77262103  1.27607986  1.71132982\n",
      " -0.36572499 -0.11991255  2.31502959 -1.08960534]\n"
     ]
    }
   ],
   "source": [
    "yhat = my_linear_model.predict(X.reshape(-1,1))\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With predictions in hand, we can then assess the quality of our model by assesing the accuracy of our predictions.  This is typically done via one of two metrics:\n",
    "\n",
    "Mean squared error: \n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "Coefficient of determination: \n",
    "\n",
    "$$r^2 = 1- \\frac{\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}{\\sum_{i=1}^n (y_i-\\bar{y})^2}$$\n",
    "\n",
    "where $\\hat{y}_i$ is our prediction for $y_i$ and where $\\bar{y}$ is the (sample) mean of $Y$.\n",
    "\n",
    "While mean squared error is often sufficient for comparing models on a given problem, it lacks a sense of scale.  $r^2$ scores address this shortcoming by scaling mean squared error by the variance of $Y$, and can thus be interpreted as the percent of variability in $Y$ explained by the model and $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 0
   },
   "source": [
    "## Discussion Question (2 pts)\n",
    "Personal data is increasingly stored in encrypted formats managed by large tech companies. In cases where a crime has occured, this data may be important to investigations by law enforcement. Apple has famously [refused to crack their encryption or build backdoors](https://www.theverge.com/2020/1/7/21054836/fbi-iphone-unlock-apple-encryption-debate-pensacola-ios-security) into their software to help law enforcement gain access to potential evidence, while [Google has charged law enforcement](https://www.nytimes.com/2020/01/24/technology/google-search-warrants-legal-fees.html) a fee to execute search warrants for user data. **What roles and responsibilities do these private tech companies have to both users and law enforcement when it comes to either sharing or safeguarding user data when a crime has occurred?**  Enter a few sentences addressing this question in the box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the properly document warrants for the government, there shouldn't preventing access to potential evidence linked to a crime, especially when the digitization of information is the new paradigm for how investigations are implemented and how evidence is retrieved. Monetization of user data without the user's consent nor allowing the user to have a stake in the monetization is an issue in itself. Monetization especially shouldn't be prevalent in criminal investigations and large tech companies should be willing to cooperate with law enforcement containing search warrants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "narrative": true
   },
   "source": [
    "# HW 5\n",
    "\n",
    "**To make it easier for TAs to grade your work, make sure that all cells are executed so that we can see your results without having to run anything**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "narrative": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression as lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "narrative": true
   },
   "source": [
    "## A: Computing Covariance, Correlation, and $r^2$ in numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 1
   },
   "source": [
    "\n",
    "\n",
    "**Problem 1 (2 points):** Below we have defined an ndarray which records paired observations of two random variables $X$,$Y$.  This ndarray is a $n$ by 2 array, where $n=15$ is the number of paired observations.  Run the code cell below, and generate a scatter plot of your data, with appropriate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "narrative": true
   },
   "outputs": [],
   "source": [
    "q = np.array(\n",
    "    [\n",
    "        [ 4.35994902, 10.84003387],\n",
    "        [ 0.25926232,  6.64625283],\n",
    "        [ 5.49662478, 12.70709295],\n",
    "        [ 4.35322393, 10.33309438],\n",
    "        [ 4.20367802, 10.21407197],\n",
    "        [ 3.30334821,  8.47051026],\n",
    "        [ 2.04648634,  7.93047576],\n",
    "        [ 6.19270966, 11.491082  ],\n",
    "        [ 2.99654674,  7.60715331],\n",
    "        [ 2.66827275,  9.15886831],\n",
    "        [ 6.21133833, 11.99190195],\n",
    "        [ 5.29142094, 10.98304651],\n",
    "        [ 1.34579945,  6.89008468],\n",
    "        [ 5.13578121,  9.01125354],\n",
    "        [ 1.84439866,  8.3941969 ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Y')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [q[i][0] for i in range(0,len(q))]\n",
    "y = [q[i][1] for i in range(0,len(q))]\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 2
   },
   "source": [
    "**Problem 2 (2 points):** Based on this plot, what kind of functional relationship do $X$ and $Y$ seem to have?  Do you think these variables are positively or negatively correlated?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is relatively positive, linear relationship between X and Y. The positive correlation derives from the increase in one value and the increase of the other value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "narrative": true
   },
   "source": [
    "Let's now compute some statistics to better understand the relationship between $X$ and $Y$.\n",
    "In the problems below, where appropriate, you will find the numpy functions `mean`, `cov`, `var`, and `corrcoef` to be useful. \n",
    "\n",
    "Remember that there are two ways to calculate variance, dividing by $N$ and dividing by $N-1$ (Note numpy and pandas have slightly different implementations of variance; one uses n-1, the other weights by n). You may use either, but be consistent. To make your code clearer, feel free to create variables for values that you use repeatedly. You can use them across cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 3
   },
   "source": [
    "**Problem 3 (1 point):** Calculate the variance of $X$ and the variance of $Y$ using numpy. **Here and in every problem below, print your results with informative labeling!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12 is the variance of X\n",
      "3.29 is the variance of Y\n"
     ]
    }
   ],
   "source": [
    "print(\"{:.2f} is the variance of X\".format(np.var(x)))\n",
    "print(\"{:.2f} is the variance of Y\".format(np.var(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 4
   },
   "source": [
    "**Problem 4 (2 pts):** Calculate the variance of $X$ and the variance of $Y$ manually (without numpy).  Note: You can still use numpy's mean function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12 is the variance of X\n",
      "3.29 is the variance of Y\n"
     ]
    }
   ],
   "source": [
    "xmean = np.mean(x)\n",
    "ymean = np.mean(y)\n",
    "\n",
    "xvar = 0\n",
    "yvar = 0\n",
    "\n",
    "for i in range(0, len(q)):\n",
    "    xvar = xvar + (x[i] - xmean)**2\n",
    "    yvar = yvar + (y[i] - ymean)**2\n",
    "xvar = xvar/len(x)\n",
    "yvar = yvar/len(y)\n",
    "\n",
    "print(\"{:.2f} is the variance of X\".format(xvar))\n",
    "print(\"{:.2f} is the variance of Y\".format(yvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 5
   },
   "source": [
    "**Problem 5 (1 point)**: Calculate the covariance of $X$ and $Y$ using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance of x,y:  3.08\n"
     ]
    }
   ],
   "source": [
    "cov = np.cov(x,y)[0,1]\n",
    "print('covariance of x,y: ', round(cov,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 6
   },
   "source": [
    "**Problem 6 (2 points):** Calculate the correlation between $X$ and $Y$ without using numpy, by combining your answers from previous problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation of x,y:  0.96\n"
     ]
    }
   ],
   "source": [
    "cr = cov/(xvar*yvar)**0.5\n",
    "print('correlation of x,y: ', round(cr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 11
   },
   "source": [
    "### B. Linear Regression\n",
    " \n",
    "Let's now test how well the relationship between $Y$ and $X$ can be described using a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 7
   },
   "source": [
    "**Problem 7 (6 points):** Using sklearn, fit a simple linear regression model to predict $Y$ from $X$. (If you're unsure of how to do this, the documentation will prove useful.)\n",
    "\n",
    "After fitting this model, print its slope and intercept, then produce a scatter plot showing the \"in sample\" predictions of this model (i.e. predictions at the points at which this this model was trained), alongside the observed values of $Y$.  Note that both scatterplots should be placed on the same graph.  Use a legend to indicate which points correspond to predictions, and which points correspond to true values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated slope:  [0.92294814]\n",
      "Estimated intercept:  6.083516562436524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1dfff965780>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linear_model = lr() \n",
    "values = linear_model.fit(np.asarray(x).reshape(-1,1),y) \n",
    " \n",
    "print('Estimated slope: ', linear_model.coef_)\n",
    "print('Estimated intercept: ',linear_model.intercept_)\n",
    "y_predict = linear_model.predict(np.asarray(x).reshape(-1,1))\n",
    "\n",
    "plt.scatter(x, y_predict)\n",
    "plt.scatter(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 8
   },
   "source": [
    "**Problem 8 (3 points):**.\n",
    "Looking at the data, a linear relationship between $Y$ and $X$ seems like a reasonable assumption.  However, linear regression also assumes that the residuals, or the differences between model predictions and observed values of $Y$, are (i) independent and (ii) identically distributed (i.e. homoskedasticity).  Let's test this assumption.  \n",
    "\n",
    "Produce another scatter plot, with observed values of $X$ on the x axis, and with the residuals $r_i = y_i - \\hat{y}_i$, where $\\hat{y}_i = f(x_i)$ is our predicted value for $x_i$ under (our linear regression model) $f$.  Based on this scatterplot, do you think the assumptions (i) and (ii) are satisfied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1dfff9c19e8>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residuals = y-y_predict\n",
    "plt.scatter(x,residuals) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't meet the homoskedasticity requirement. As the value of x increases, the magnitudes of the residuals progressively increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 9
   },
   "source": [
    "**Problem 9 (3 points):** \n",
    "\n",
    "Finally, let's quantify how well our linear regression modeled the data.  Compute and print the coefficient of determination (i.e. $r^2$) for this model, on this data **using the formula below (NOT via `numpy` or `pandas` `.corr()` method)**:\n",
    "\n",
    "$$r^2 = 1 - \\frac{SSR}{SST}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$SSR = \\sum_{i=1}^n r_i^2$$\n",
    "\n",
    "$$SST = \\sum_{i=1}^n (y_i-\\bar{y})^2$$\n",
    "\n",
    "Note: Sometimes the terminology $r^2 = 1 - \\frac{MSE(y,\\hat{y})}{Var(y)}$ is used, where MSE = mean squared error. Ponder why these definitions are equivalent, when using $N$ in the denominator of your variance calculation.  In the future, you can automatically perform this calculation using `LinearRegression`'s built in `score` method, or by calling `sklearn.metrics.r2_score`.\n",
    "\n",
    "Based on your findings, how well is $Y$ explained by $X$ in the context of a our model (be precise and quantitative in your answer).  How does this $r^2$ score relate to the corr($X,Y$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination:  0.807\n"
     ]
    }
   ],
   "source": [
    "SSR = sum(residuals**2)\n",
    "SST = sum((y-ymean)**2)\n",
    "coeff_det = 1-SSR/SST\n",
    "\n",
    "print('Coefficient of determination: ', round(coeff_det,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A less than 20% variance in Y over X implies that Y is explained relatively effectively. The square root of this value gives about 0.9 which is close to the correlation calculated correlation of 0.96. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "narrative": true
   },
   "source": [
    "## C:  Making data with a desired correlation\n",
    "\n",
    "To help you better understand correlation, you'll now create datasets that have a prescribed correlation.\n",
    "\n",
    "Note that in practice, due to randomness, it will be hard to get exact results.  Instead, round your empirical correlations to two decimal places and, whenever relevent, be sure to generate lots (10000+) of random variables to ensure reproducible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 10
   },
   "source": [
    "\n",
    "**Problem 10 (1 point):** Create two arrays, x, y, whose correlation is 1.0. **Calculate $Corr(x,y)$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient coefficient of x and y:  1.0\n"
     ]
    }
   ],
   "source": [
    "x = np.round(10*np.random.rand(10000),2)\n",
    "y = 2*x - 4\n",
    "corr = np.round(np.corrcoef(x,y)[0,1],2)\n",
    "print('Coefficient coefficient of x and y: ', corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 11
   },
   "source": [
    "\n",
    "**Problem 11 (1 point):** Using the same x values from your answer to  problem 10, create a second array, `y2`, that also has perfect correlation with `x`, but which is not identical to your `y` from the previous problem. **Print your observed correlation coefficient between `x` and `y2`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient of x and y2:  1.0\n"
     ]
    }
   ],
   "source": [
    "y2 = x/4 + 20\n",
    "corr2 = np.round(np.corrcoef(x,y2)[0,1],2)\n",
    "print('Correlation coefficient of x and y2: ', corr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 12
   },
   "source": [
    "\n",
    "**Problem 12 (1 point):** Create two arrays with correlation close to 0, which are independent of each other, and print the correlation of your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient of A and B:  -0.01\n"
     ]
    }
   ],
   "source": [
    "A = np.random.normal(0,1,10000)\n",
    "B = np.random.normal(0,1,10000)\n",
    "cor = np.round(np.corrcoef(A,B)[0,1],2)\n",
    "print('Correlation coefficient of A and B: ', cor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 13
   },
   "source": [
    "**Problem 13 (3 points):** Create two arrays with correlation **$r = $** 0.5 (+/- 0.01) by adding noise. First create $X$. Then create a new array `noise` of the same length using `numpy.random.normal()` with mean zero and standard deviation 1.0. Hint: Set a variable `scale`, initially equal to 1.0. Let $Y$ equal $X$ plus `scale` times `noise` and calculate $r$. If $r$ is greater than or less than 0.5, change the value of `scale` and repeat. It's OK to do this by brute force, or by figuring out how to search for the correct value of `noise` in a computationally efficient way.  **Print your scale factor and $r$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient of X and Y:  0.5\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(0,1,10000)\n",
    "scale = 1.75\n",
    "noise = np.random.normal(0,1,10000)\n",
    "\n",
    "Y = X + scale*noise\n",
    "\n",
    "cor = np.round(np.corrcoef(X,Y)[0,1],2)\n",
    "print('Correlation coefficient of X and Y: ', cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": 14
   },
   "source": [
    "**Problem 14 (3 points):** Two variables which are independent have 0 correlation, but the reverse statement is not always true.  Create an array $X$, and then generate an array $Y$ that is a deterministic function of $X$, but where the correlation between $X$ and $Y$ is near 0. In other words, given $X$, you know *exactly* what $Y$ will be, but the $r$  value is still close to 0.  Hint: You've seen an example of this in lecture (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#/media/File:Correlation_examples2.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient of X and Y:  -0.0\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(0,1,10000)\n",
    "Y = (1-X**2)**2\n",
    "cor = np.round(np.corrcoef(X,Y)[0,1],2)\n",
    "print('Correlation coefficient of X and Y: ', cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
